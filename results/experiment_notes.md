# Obsidian RAG 実験記録

## 実験概要
- **実験日時**: 2025-07-19
- **実験者**:
- **目的**: ローカル埋め込みモデル（BGE-M3）とクラウド埋め込みモデル（Gemini）の検索精度・パフォーマンス比較
- **実験環境**:
  - OS: macOS (Darwin)
  - CPU: Apple Silicon
  - GPU: MPS (Metal Performance Shaders)
  - Memory:

## インデックス作成統計

### 共通データ統計
- **処理ファイル数**: 182
- **Vault総サイズ**: 約2.5MB
- **トークナイザー**: tiktoken (cl100k_base) - 両モデル共通
- **推定元トークン数（オーバーラップ除く）**:
  - BGE-M3: 395,910トークン
  - Gemini: 372,908トークン
  - 差異はチャンク数の違いによる計算式の影響
- **総処理トークン数（オーバーラップ含む）**:
  - BGE-M3: 434,910トークン
  - Gemini: 446,108トークン

### パフォーマンス比較（複数実行）

#### BGE-M3
| 実行日時 | 総実行時間 | 生成チャンク数 | チャンク処理速度 | ファイル処理速度 |
|----------|-----------|--------------|----------------|----------------|
| [2025-07-19 02:46:20](./indexing_stats_bgem3_20250719_024620.md) | 347.09秒 (5.8分) | 377 | 1.09 chunks/秒 | 0.52 files/秒 |
| [2025-07-19 03:32:36](./indexing_stats_bgem3_20250719_033236.md) | 350.56秒 (5.8分) | 377 | 1.08 chunks/秒 | 0.52 files/秒 |
| [2025-07-19 05:03:52](./indexing_stats_bgem3_20250719_050352.md) | 352.66秒 (5.9分) | 377 | 1.07 chunks/秒 | 0.52 files/秒 |
| **平均** | **350.10秒 (5.8分)** | **377** | **1.08 chunks/秒** | **0.52 files/秒** |

**設定 [config.yaml](../config/config.yaml)**:
- デバイス: MPS (GPU)
- バッチサイズ: 8
- 最大チャンクサイズ: 3000 tokens
- ベクトル次元数: 1024
- 平均チャンク数/ファイル: 2.1

**トークン統計（実測）**:
- 総処理トークン数: 434,910
- 推定元トークン数: 395,910
- 平均トークン数/チャンク: 1,154

#### Gemini（クォータ引き上げ後: 300,000トークン/分）
| 実行日時 | 総実行時間 | 生成チャンク数 | チャンク処理速度 | ファイル処理速度 | API処理割合 |
|----------|-----------|--------------|----------------|----------------|------------|
| [2025-07-19 04:48:46](./indexing_stats_gemini_20250719_044846.md) | 138.02秒 (2.3分) | 548 | 3.97 chunks/秒 | 1.32 files/秒 | 99.6% |
| [2025-07-19 04:54:19](./indexing_stats_gemini_20250719_045419.md) | 169.98秒 (2.8分) | 548 | 3.22 chunks/秒 | 1.07 files/秒 | 99.7% |
| [2025-07-19 04:57:45](./indexing_stats_gemini_20250719_045745.md) | 150.16秒 (2.5分) | 548 | 3.65 chunks/秒 | 1.21 files/秒 | 99.7% |
| **平均** | **152.72秒 (2.5分)** | **548** | **3.61 chunks/秒** | **1.20 files/秒** | **99.7%** |

**設定 [config.yaml](../config/config.yaml)**:
- 並列リクエスト数: 5
- バッチサイズ: 10
- レート制限遅延: 0.1秒
- 最大チャンクサイズ: 1500 tokens
- ベクトル次元数: 768
- 平均チャンク数/ファイル: 3.0

**トークン統計（実測）**:
- 総処理トークン数: 446,108
- 推定元トークン数: 372,908
- 平均トークン数/チャンク: 814

### インデックス作成の比較分析
- **速度**: Geminiは約2.3倍高速（BGE-M3: 5.8分、Gemini: 2.5分）
- **チャンク数**: Geminiの方が45%多い（BGE-M3: 377、Gemini: 548）
  - 理由: 最大チャンクサイズの違い（BGE-M3: 3000、Gemini: 1500）
- **トークン処理量**:
  - 推定元トークン数: BGE-M3 395,910、Gemini 372,908（チャンク数差による計算上の違い）
  - 処理トークン数: BGE-M3 434,910、Gemini 446,108（オーバーラップ含む）
  - 平均トークン/チャンク: BGE-M3 1,154、Gemini 814
- **API呼び出しの詳細**（Geminiのみ）:
  - 並列数5、バッチサイズ10で最大50リクエスト同時処理
  - API処理時間が全体の99.7%を占める
  - クォータ引き上げ（200,000→300,000トークン/分）により429エラーを解消
  - 実行時間のばらつきは2回目の実行で1ドキュメント欠損したことによる
- **安定性**:
  - BGE-M3: 実行時間が安定（標準偏差約3秒）
  - Gemini: 実行時間にややばらつきあり（最大32秒差）

## 実験結果サマリー

### 検索性能比較（ベンチマーク結果より）
| モデル | 平均埋め込み時間 | 平均検索時間 | 合計時間 | 備考 |
|--------|------------------|--------------|----------|------|
| BGE-M3 | 0.34秒 | 0.06秒 | 0.40秒 | 高速だが言語横断検索が弱い |
| Gemini | 1.40秒 | 1.43秒 | 2.83秒 | 言語横断検索に優れる |

### RAG回答性能比較（日本語クエリ「JETLSのincremental analysis system」）
| 構成 | 平均回答生成時間 | 平均合計時間 | 備考 |
|------|------------------|--------------|------|
| Gemini + Gemini Flash | 14.6秒 | 17.5秒 | 最も安定した性能 |
| Gemini + Gemma3n | 38.6秒 | 41.4秒 | ローカルLLMでは高速な部類 |
| BGE-M3 + Gemini Flash | 14.1秒 | 14.5秒 | **最速構成** |
| BGE-M3 + Gemma3n | 57.0秒 | 57.4秒 | 完全ローカル構成 |

### インクリメンタル更新性能
| モデル | ファイル | チャンク数 | 平均更新時間 | DB処理時間 | API時間 | 備考 |
|--------|----------|-----------|--------------|-----------|---------|------|
| BGE-M3 | Incremental analysis system.md | 1→1 | 0.85秒 | 0.82秒 | N/A | 単一チャンクの高速更新 |
| BGE-M3 | JETLS meeting time.md | 10→10 | 6.27秒 | 6.24秒 | N/A | 10チャンクでも線形的な処理時間 |
| Gemini | Incremental analysis system.md | 2→2 | 1.62秒 | 0秒 | 2.26秒 | API処理が主要因 |
| Gemini | JETLS meeting time.md | 17→17 | 4.94秒 | 0秒 | 16.48秒 | 多チャンクで効率的な並列処理 |

## 詳細な観察

### 検索精度に関する観察
- **日本語クエリ**:
- **英語クエリ**:
- **コンテキスト依存クエリ**:
- **技術用語検索**:

### 回答品質に関する観察
- **Gemini Flash**:
  - 日本語クエリ: 14-15秒で安定
  - 英語クエリ: 8-9秒（約40%高速）
  - エラー率: 第2回実行で0%
- **Gemma3n (Ollama)**:
  - 日本語クエリ: 35-40秒
  - 英語クエリ: 45-57秒（日本語より遅い）
  - 非常に安定した処理時間
- **埋め込みモデルによる違い**:
  - BGE-M3使用時の方が総合的に高速（埋め込み生成が7倍速い）
  - 検索精度の差は回答時間に大きく影響しない

### パフォーマンス特性
- **BGE-M3の特徴**:
  - 埋め込み生成: 0.3-0.4秒（非常に高速）
  - MPS（Apple Silicon GPU）使用で安定
  - 初回実行時のレイテンシあり（モデルロード）
  - メモリ効率的（FP16使用）
  - インクリメンタル更新: チャンク数に比例（0.6秒/チャンク）
- **Geminiの特徴**:
  - 埋め込み生成: 1.4秒前後（API呼び出しのオーバーヘッド）
  - ネットワーク依存で若干のばらつき
  - 並列処理により大規模データでは効率的
  - クォータ管理が必要（300,000トークン/分）
  - インクリメンタル更新: 並列処理で高速化（17チャンクで約5秒）
- **ボトルネック**:
  - RAGシステム全体: 回答生成が90%以上の時間を占める
  - Gemini: API呼び出しのレイテンシ
  - BGE-M3: 初回のモデルロード時間

### インクリメンタル更新の特性
- **BGE-M3**:
  - DB処理時間が支配的（95%以上）
  - チャンク数に比例した線形的な処理時間
  - 1チャンク: 0.85秒、10チャンク: 6.27秒
  - ローカル処理のため安定性が高い
- **Gemini**:
  - API処理時間が支配的だが、実際の更新は高速
  - 並列処理により効率的（チャンク数が増えても処理時間の増加が緩やか）
  - 2チャンク: 1.62秒、17チャンク: 4.94秒（約3倍の速度効率）
  - ネットワーク状況に依存

## 結論と推奨事項

### 使い分けの指針
- **BGE-M3を推奨する場合**:
  - 完全ローカル環境が必須（プライバシー重視）
  - 単一言語（日本語のみ、英語のみ）での検索
  - API料金を避けたい場合
  - 小規模なインクリメンタル更新が頻繁な場合
  - リアルタイム性を重視する場合（検索速度0.06秒）
- **Geminiを推奨する場合**:
  - 日本語・英語混在環境での検索
  - 言語横断検索が必要な場合
  - 検索精度を最優先する場合
  - インデックス作成速度を重視（2.3倍高速）
  - 大規模なファイル更新が多い場合（並列処理の恩恵）

### 今後の改善案
1. **BGE-M3の言語横断検索改善**
   - クエリ拡張（日本語→英語キーワード追加）
   - メタデータの強化
   - より小さなチャンクサイズの検討

2. **性能最適化**
   - 回答生成モデルのキャッシング
   - BGE-M3のモデル事前ロード
   - バッチ処理の最適化

3. **ハイブリッドアプローチ**
   - 言語検出による自動モデル選択
   - BGE-M3（速度）+ Gemini（精度）の組み合わせ
   - リランキングの実装

## 実験データ
- ベンチマーク結果: 
  - 検索・回答性能: `results/benchmark_results_20250719_064953.json`
  - インクリメンタル更新: `results/benchmark_results_20250719_134714.json`, `results/benchmark_results_20250719_134836.json`
- 詳細分析: 
  - `notes/benchmark-comparison-analysis.md`
  - `notes/cross-language-search-performance.md`
